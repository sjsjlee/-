# -*- coding: utf-8 -*-
"""따릉이 예측 경진_final

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZjKXXpMU9nF3qflXz-WZH4YHJMTlWbri
"""

import pandas as pd
import numpy as np
import warnings



from google.colab import drive
drive.mount('/content/drive')

warnings.filterwarnings('ignore')

bicycle = pd.read_csv('/content/drive/MyDrive/dataset/train.csv')

bicycle.head()

bicycle.columns

def check_missing_col(dataframe):
    counted_missing_col = 0
    for i, col in enumerate(bicycle.columns):
        missing_values = sum(bicycle[col].isna())
        is_missing = True if missing_values >= 1 else False
        if is_missing:
            counted_missing_col += 1
            print(f'결측치가 있는 컬럼은: {col}입니다')
            print(f'총 {missing_values}개의 결측치가 존재합니다.')

        if i == len(bicycle.columns) - 1 and counted_missing_col == 0:
            print('결측치가 존재하지 않습니다')

check_missing_col(bicycle)

# 결측치 존재하지 않음

bicycle.isnull().sum()

def separate_datetime(dataframe):
  year = []
  month = []
  day = []
  
  for date in dataframe.date_time :
    year_point, month_point, day_point = date.split('-')
    year.append(int(year_point))
    month.append(int(month_point))
    day.append(int(day_point))
  return year, month, day

year, month, day = separate_datetime(bicycle)

bicycle['year'] = year
bicycle['month'] = month
bicycle['day'] = day

bicycle.head()

X = bicycle.drop(['date_time', 'number_of_rentals'], axis = 1)
y = bicycle.number_of_rentals

# 선형회귀분석 모델

from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X, y)

y_hat = model.predict(X)

name = np.mean(abs(y_hat -y)/y)

name

import matplotlib.pyplot as plt

plt.style.use('ggplot')
plt.figure(figsize = (20,10))
plt.plot(y_hat, label = 'prediction')
plt.plot(y, label = 'real')
plt.legend(fontsize = 20)
plt.show()



# 새 변수 추가 - 요일
import datetime as dt

bicycle['date_time'] = pd.to_datetime(bicycle['date_time'])
bicycle['day_of_week'] = bicycle['date_time'].dt.day_name() #문자형 - 그래프 통해 확인

# 새변수 추가 - 불쾌지수
temperature = bicycle['high_temp']
humidity = bicycle['humidity']

bicycle['discomfort_index'] = 1.8*temperature-0.55*(1-humidity/100)*(1.8*temperature-26)+32

import seaborn as sns

sns.scatterplot(data = bicycle, x= 'discomfort_index', y = 'number_of_rentals')

import matplotlib.pyplot as plt

plt.figure(figsize = (10,10))
bar = sns.barplot(data = bicycle, x = 'day_of_week', y = 'number_of_rentals') #요일과 렌탈수와 별로 연관이 없음

bicycle = bicycle.drop(['day_of_week'], axis =1)

sns.barplot(data = bicycle, x= 'year', y ='number_of_rentals')

#연도별로 꾸준히 증가함을 알 수 있음

# 선형회귀분석 유의한 변수 확인하기

import statsmodels.api as sm

# 변수 지정
x = bicycle[['wind_direction', 'sky_condition', 'precipitation_form',
       'wind_speed', 'humidity', 'low_temp', 'high_temp',
       'Precipitation_Probability', 'year', 'month', 'day']]
target = bicycle.number_of_rentals

x_data = sm.add_constant(x, has_constant = "add")
ln_reg = sm.OLS(target, x_data).fit()
ln_reg.summary()

# 0.1 수준에서 유의한 변수들 : const, wind_direction, wind_speed, high_temp, year, month
# 확실히 제외할 변수들 : sky_condition, humidity, day, Precipitation_Probability

# 변수 추가 타당성 검증 -> 수정된 r-squared 값 증가함, f통계량 유의함
 
x = bicycle[['wind_direction', 'sky_condition', 'precipitation_form',
       'wind_speed', 'humidity', 'low_temp', 'high_temp',
       'Precipitation_Probability', 'year', 'month', 'day', 'discomfort_index']]
target = bicycle.number_of_rentals

x_data = sm.add_constant(x, has_constant = "add")
ln_reg = sm.OLS(target, x_data).fit()
ln_reg.summary()

# 유의한 변수들 : wind_direction, wind_speed, humidity, year, month, discomfot_index
# 애매한 변수들 : low_temp, precipitation_probablity, day

# 제외하고 다시 선형회귀분석

x = x.drop(['sky_condition', 'precipitation_form', 'day'], axis =1)
target = bicycle.number_of_rentals

x_data = sm.add_constant(x, has_constant = "add")
ln_reg = sm.OLS(target, x_data).fit()
ln_reg.summary()

# low_temp 제외하고 어느정도 유의함을 확인할 수 있음 : wind_direction, precipitation_probability, wind_speed, humidity, high_temp, year, month 선택

#상관관계 분석

import scipy.stats as stats

for item in X:
  print(item)
  x = bicycle[item].values 
  print('Correlation : {:.2f}'.format(stats.pearsonr(x, y)[0]))
  print('P-value : {:.4f}'.format(stats.pearsonr(x, y)[1]))
  print('\n')

  '''
  0.2 이상 : wind_direction
  0.3 이상 : sky_condition, low_temp, month
  0.4 이상 : precipitation_form, wind_speed, high_temp, Precipitation_Probability, discomfot
  0.7 이상 : year
  0.2 이하 , 제외 : humidity, day
  '''

# 후진소거법

import statsmodels.api as sm

variables = bicycle.drop(['date_time', 'number_of_rentals'], axis=1).columns.tolist()
selected_variables = []
alpha = 0.1

df = bicycle.drop(['date_time'], axis = 1)
sv_per_steps = []
adjusted_r_squared = []
steps = []
step = 0

sv_per_step = [] ## 각 스텝별로 선택된 변수들
adjusted_r_squared = [] ## 각 스텝별 수정된 결정계수
steps = [] ## 스텝
step = 0
while len(variables) > 0:
    remainder = list(set(variables) - set(selected_variables))
    pval = pd.Series(index=remainder) ## 변수의 p-value
    for col in remainder: 
        X = df[selected_variables+[col]]
        X = sm.add_constant(X)
        model = sm.OLS(y,X).fit()
        pval[col] = model.pvalues[col]
 
    min_pval = pval.min()
    if min_pval < alpha: ## 최소 p-value 값이 기준 값보다 작으면 포함
        selected_variables.append(pval.idxmin())
        
        step += 1
        steps.append(step)
        adj_r_squared = sm.OLS(y,sm.add_constant(df[selected_variables])).fit().rsquared_adj
        adjusted_r_squared.append(adj_r_squared)
        sv_per_step.append(selected_variables.copy())
    else:
        break

selected_variables

'''
선형회귀분석 유의 변수 : wind_direction, precipitation_probability, wind_speed, humidity, high_temp, year, month, discomfort_index
상관관계 0.2 이상 변수 : wind_direction, sky_condition, low_temp, month, precipitation_form, wind_speed, high_temp, Precipitation_Probability, discomfort_index, year 
후진소거법 선택변수 : 'year', 'high_temp', 'precipitation_form', 'wind_speed', 'wind_direction', 'month', 'discomfort_index','humidity','Precipitation_Probability'

공통 변수 : wind_direction, wind_speed, high_temp, discomfort_index, year, month
2개이상 포함 변수 : 'Precipitation_Probability', 'precipitation_form' ->추가해보면서 확인해보기

"""* 선형회귀분석 유의 변수 : wind_direction, precipitation_probability, wind_speed, humidity, high_temp, year, month, discomfort_index

* 상관관계 0.2 이상 변수 : wind_direction, sky_condition, low_temp, month, precipitation_form, wind_speed, high_temp, Precipitation_Probability, discomfort_index, year 

* 후진소거법 선택변수 : 'year', 'high_temp', 'precipitation_form', 'wind_speed', 'wind_direction', 'month', 'discomfort_index','humidity','Precipitation_Probability'

* 공통 변수 : wind_direction, wind_speed, high_temp, discomfort_index, year, month
* 2개이상 포함 변수 : 'Precipitation_Probability', 'precipitation_form' ->추가해보면서 확인해보기
"""

#1차 변수

X = bicycle[['wind_direction', 'wind_speed', 'Precipitation_Probability', 'high_temp', 'discomfort_index', 'year', 'month']]
Y = bicycle.number_of_rentals

#2차변수('Precipitation_Probability' 추가) ->이게가장설명력좋음

X = bicycle[['wind_direction', 'wind_speed', 'Precipitation_Probability', 'high_temp', 'discomfort_index', 'year', 'month']]
Y = bicycle.number_of_rentals

# 3차변수('precipitation_form' 추가)

X = bicycle[['wind_direction', 'wind_speed', 'precipitation_form', 'high_temp', 'discomfort_index', 'year', 'month']]
Y = bicycle.number_of_rentals

#4차변수('precipitation_form', 'Precipitation_Probability' 추가)

X = bicycle[['wind_direction', 'wind_speed', 'precipitation_form', 'Precipitation_Probability', 'high_temp', 'discomfort_index', 'year', 'month']]
Y = bicycle.number_of_rentals

#test_train_split

from sklearn.model_selection import train_test_split


X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 10)

colnames = X_train.columns
new_colnames = [i for i in colnames]

# 데이터 전처리 - MinmaxScaler

from sklearn.preprocessing import MinMaxScaler
mm_scaler = MinMaxScaler()

mm_scaler.fit(X_train)
X_train_scaled = mm_scaler.transform(X_train)
X_test_scaled = mm_scaler.transform(X_test)

minmax_Xtrain = pd.DataFrame(X_train_scaled, index = X_train.index, columns = new_colnames)
minmax_Xtest = pd.DataFrame(X_test_scaled, index = X_test.index, columns = new_colnames)

## RobustScaler -> 선택

from sklearn.preprocessing import RobustScaler
rob_scaler = RobustScaler()

rob_scaler.fit(X_train)
X_train_robust = rob_scaler.transform(X_train)
X_test_robust = rob_scaler.transform(X_test)

robust_Xtrain = pd.DataFrame(X_train_robust, index = X_train.index, columns = new_colnames)
robust_Xtest = pd.DataFrame(X_test_robust, index = X_test.index, columns = new_colnames)

## StandardScaler

from sklearn.preprocessing import StandardScaler
standard_scaler = StandardScaler()

standard_scaler.fit(X_train)
X_train_st = standard_scaler.transform(X_train)
X_test_st = standard_scaler.transform(X_test)

st_Xtrain = pd.DataFrame(X_train_st, columns = new_colnames, index = X_train.index)
st_Xtest = pd.DataFrame(X_test_st, columns = new_colnames, index = X_test.index)

## MaxAbsScaler

from sklearn.preprocessing import MaxAbsScaler
abs_scaler = MaxAbsScaler()

abs_scaler.fit(X_train)
X_train_abs = abs_scaler.transform(X_train)
X_test_abs = abs_scaler.transform(X_test)

abs_Xtrain = pd.DataFrame(X_train_abs, columns = new_colnames, index = X_train.index)
abs_Xtest = pd.DataFrame(X_test_abs, columns = new_colnames, index = X_test.index)



#XGBRegressor

from xgboost import XGBRegressor # 0.33

xgb_model = XGBRegressor(max_depth = 5, n_estimators = 1500, learning_rate= 0.1, alpha = 10) #objective='reg:squarederror'
xgb_model.fit(minmax_Xtrain, Y_train)

#random forest regression

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(max_depth = 5, n_estimators = 1000)
rf_model.fit(minmax_Xtrain, Y_train)

# 그래디언트 부스팅 - standard

from sklearn.ensemble import GradientBoostingRegressor

standard_model = GradientBoostingRegressor(random_state = 9, max_depth = 5, n_estimators = 1500, learning_rate= 0.1)
standard_model.fit(st_Xtrain, Y_train)

# 그래디언트 부스팅 - minmax

from sklearn.ensemble import GradientBoostingRegressor

minmax_model = GradientBoostingRegressor(random_state = 9, max_depth = 5, n_estimators = 1500, learning_rate= 0.1)
minmax_model.fit(minmax_Xtrain, Y_train)

_# 그래디언트 부스팅 - robust

from sklearn.ensemble import GradientBoostingRegressor

robust_model = GradientBoostingRegressor(random_state = 9, max_depth = 5, n_estimators = 1500, learning_rate= 0.1)
robust_model.fit(robust_Xtrain, Y_train)

# 그래디언트 부스팅 - maxabs

from sklearn.ensemble import GradientBoostingRegressor

maxabs_model = GradientBoostingRegressor(random_state = 9, max_depth = 5, n_estimators = 1500, learning_rate= 0.1)
maxabs_model.fit(abs_Xtrain, Y_train)



xgb_y_pred = xgb_model.predict(X_test)

rf_y_pred = rf_model.predict(X_test)

mm_y_pred = minmax_model.predict(minmax_Xtest)

st_y_pred = standard_model.predict(st_Xtest)

rob_y_pred = robust_model.predict(robust_Xtest)

abs_y_pred = maxabs_model.predict(abs_Xtest)



nmae = np.mean((xgb_y_pred -Y_test)/Y_test)

nmae

nmae = np.mean((rf_y_pred -Y_test)/Y_test)

nmae

nmae = np.mean(abs(mm_y_pred -Y_test)/Y_test)

nmae

nmae = np.mean(abs(st_y_pred -Y_test)/Y_test)

nmae

nmae = np.mean(abs(rob_y_pred -Y_test)/Y_test) #근소한 차이로 robust scaler 선택

nmae #근소한 차이로 robust scaler 선택

nmae = np.mean(abs(abs_y_pred -Y_test)/Y_test)

nmae



rob_y_pred

#Y_test의 array화

y_test = Y_test.to_numpy()

import matplotlib.pyplot as plt

plt.style.use('ggplot')
plt.figure(figsize = (20,10))
plt.plot(rob_y_pred, label = 'prediction')
plt.plot(y_test, label = 'true')
plt.legend(fontsize = 20)
plt.show()



from sklearn.model_selection import GridSearchCV

parameters = {'max_depth' : [3,4,5,6], 'random_state' : [7,8,9,10]}

model = GradientBoostingRegressor()
grid_dtree = GridSearchCV(model, param_grid = parameters, cv = 3, refit=True)

grid_dtree.fit(X_train, Y_train)

scores_df = pd.DataFrame(grid_dtree.cv_results_)

from sklearn.model_selection import GridSearchCV

parameters = {'max_depth' : [5,6,7], 'random_state' : [9, 10, 11]}

model = GradientBoostingRegressor()
grid_dtree = GridSearchCV(model, param_grid = parameters, cv = 3, refit=True)

grid_dtree.fit(robust_Xtrain, Y_train)

scores_df = pd.DataFrame(grid_dtree.cv_results_)

scores_df = scores_df[['params', 'mean_test_score', 'rank_test_score', 
           'split0_test_score', 'split1_test_score', 'split2_test_score']]

scores_df.sort_values(by = 'rank_test_score').head()



# LSTM -> 학습률 떨어짐

x_train = minmax_Xtrain.values
x_test = minmax_Xtest.values
y_train = Y_train.to_numpy()
y_test = Y_test.to_numpy()

x_train_t = x_train.reshape(x_train.shape[0], 7, 1)
x_test_t = x_test.reshape(x_test.shape[0], 7, 1)
y_train_t = y_train.reshape(y_train.shape[0], 1)
y_test_t = y_test.reshape(y_test.shape[0], 1)

from keras.layers import LSTM
from keras.models import Sequential
from keras.layers import Dense
import keras.backend as K
from keras.callbacks import EarlyStopping

K.clear_session()
model = Sequential()
model.add(LSTM(20, input_shape = (x_train_t.shape[1],x_train_t.shape[2]), activation = 'relu', return_sequences= False))
model.add(Dense(1)) #output = 1
model.compile(loss='mean_absolute_error', optimizer='adam')
model.summary()

early_stop = EarlyStopping(monitor = 'loss', patience = 1, verbose =1)

model.fit(x_train_t, y_train_t, epochs = 100,
          batch_size = 1000, verbose =  1, callbacks = [early_stop])

score = model.evaluate(x_test_t, y_test_t, batch_size = 30)

Y_pred = model.predict(x_test_t)

import matplotlib.pyplot as plt

plt.style.use('ggplot')
plt.figure(figsize = (20,10))
plt.plot(Y_pred, label = 'prediction')
plt.plot(y_test_t, label = 'true')
plt.legend(fontsize = 20)
plt.show()





#테스트 데이터셋에 적용

test_df = pd.read_csv("/content/drive/MyDrive/dataset/test.csv")
submission = pd.read_csv("/content/drive/MyDrive/dataset/sample_submission.csv")


year, month, day = separate_datetime(test_df)

test_df['year'] = year
test_df['month'] = month
test_df['day'] = day

test_df.head()

temp = test_df['high_temp']
humid = test_df['humidity']

test_df['discomfort_index'] = 1.8*temp-0.55*(1-humid/100)*(1.8*temp-26)+32

test_X = test_df[['wind_direction', 'wind_speed', 'Precipitation_Probability', 'high_temp', 'discomfort_index', 'year', 'month']]

test_X_transformed = rob_scaler.transform(test_X)

colnames = test_X.columns
new_colnames = [i for i in colnames]

X_test_transformed = pd.DataFrame(test_X_transformed, index = test_X.index, columns = new_colnames)

test_Y_pred = robust_model.predict(X_test_transformed)

submission['number_of_rentals'] = test_Y_pred

submission.to_csv('/content/drive/MyDrive/dataset/submission_final.csv', index=False)

test_Y_pred

